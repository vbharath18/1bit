{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08647ea2",
   "metadata": {},
   "source": [
    "# BitNet Model Implementation with Performance Optimizations\n",
    "\n",
    "This notebook demonstrates how to load and run the BitNet model with various optimizations for improved performance. We'll walk through each step of the process, from setting up the environment to generating and evaluating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b8f56b0",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-req-build-8rugimf1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-req-build-8rugimf1\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 31f8a0fe8a7e2db1ee30bf32ed5976cd11f3283c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install torch git+https://github.com/huggingface/transformers.git bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c932d",
   "metadata": {},
   "source": [
    "## 1. Set Environment Variables\n",
    "\n",
    "First, we'll set up the necessary environment variables for tokenizer parallelism and PyTorch CPU memory alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25687c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set tokenizer parallelism before importing tokenizer-related modules\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Enable memory alignment\n",
    "os.environ['PYTORCH_CPU_ALLOC_CONF'] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d05296",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import the necessary Python libraries and enable PyTorch optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad26db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitNetConfig\n",
    "import torch.backends.cpu\n",
    "import time\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cpu.optimize = True\n",
    "torch.set_num_threads(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505ea07",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Model\n",
    "\n",
    "Load the BitNet tokenizer and set up padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "526780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/bitnet-b1.58-2B-4T\"\n",
    "\n",
    "# Load tokenizer and set up padding token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb33ef",
   "metadata": {},
   "source": [
    "## 4. Configure BitNet Model\n",
    "\n",
    "Set up the model configuration with various optimizations for improved inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5ec19c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BitNetForCausalLM(\n",
       "    (model): BitNetModel(\n",
       "      (embed_tokens): Embedding(128256, 2560, padding_idx=128009)\n",
       "      (layers): ModuleList(\n",
       "        (0-29): 30 x BitNetDecoderLayer(\n",
       "          (self_attn): BitNetAttention(\n",
       "            (q_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (k_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
       "            (v_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
       "            (o_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (attn_sub_norm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "          )\n",
       "          (mlp): BitNetMLP(\n",
       "            (gate_proj): AutoBitLinear(in_features=2560, out_features=6912, bias=False)\n",
       "            (up_proj): AutoBitLinear(in_features=2560, out_features=6912, bias=False)\n",
       "            (down_proj): AutoBitLinear(in_features=6912, out_features=2560, bias=False)\n",
       "            (act_fn): ReLUSquaredActivation()\n",
       "            (ffn_sub_norm): BitNetRMSNorm((6912,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "          (post_attention_layernorm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "      (rotary_emb): BitNetRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize BitNet configuration with optimizations\n",
    "config = BitNetConfig.from_pretrained(model_id)\n",
    "config.low_cpu_mem_usage = True\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Performance optimizations\n",
    "config.use_cache = True  # Enable KV-cache for faster inference\n",
    "config.pretraining_tp = 1  # Tensor parallelism degree\n",
    "config.max_position_embeddings = 2048  # Match with your max_length\n",
    "config.hidden_dropout_prob = 0  # Disable dropout for inference\n",
    "config.attention_dropout_prob = 0  # Disable attention dropout for inference\n",
    "config.use_memory_efficient_attention = True  # Use memory efficient attention\n",
    "config.scale_attention_softmax_in_fp32 = False  # Keep in lower precision\n",
    "config.use_flash_attention = True  # Enable flash attention if supported\n",
    "\n",
    "# Load model with optimized configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,  # Explicitly set dtype to avoid ValueError\n",
    "    device_map='auto'  # Automatically handle device placement\n",
    ")\n",
    "\n",
    "# Move model to CPU and optimize\n",
    "model = model.cpu()\n",
    "model = torch.compile(model)  # Enable torch.compile for faster execution\n",
    "model.eval()  # Set to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e6f65",
   "metadata": {},
   "source": [
    "## 5. Prepare Input Messages\n",
    "\n",
    "Define the chat messages and apply the chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80047deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for processing\n",
    "BATCH_SIZE = 1  # Adjust based on your CPU memory\n",
    "\n",
    "# Prepare chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the latest advancements in AI.\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037682ec",
   "metadata": {},
   "source": [
    "## 6. Tokenize Input\n",
    "\n",
    "Tokenize the input messages and measure the tokenization time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55b7e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():  # More efficient than no_grad for inference\n",
    "    # Measure tokenization time\n",
    "    tokenization_start = time.time()\n",
    "    chat_input = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust based on your needs\n",
    "    )\n",
    "    tokenization_time = time.time() - tokenization_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0511195",
   "metadata": {},
   "source": [
    "## 7. Generate Response\n",
    "\n",
    "Generate the model response with optimized generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0eb21e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharathvjs/CodeSpace/1bit/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:684: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    # Measure generation time\n",
    "    generation_start = time.time()\n",
    "    chat_outputs = model.generate(\n",
    "        **chat_input,\n",
    "        max_new_tokens=500,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # Slightly higher temperature for faster sampling\n",
    "        top_p=0.95,      # Slightly higher top_p for faster sampling\n",
    "        top_k=40,        # Add top_k sampling for better speed/quality balance\n",
    "        use_cache=True,  # Enable KV-cache\n",
    "        num_beams=1,     # Disable beam search for faster generation\n",
    "        early_stopping=True,  # Enable early stopping\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,  # Light penalty to avoid repetitions efficiently\n",
    "        length_penalty=1.0,  # Neutral length penalty for faster completion\n",
    "        no_repeat_ngram_size=3  # Prevent repetition of 3-grams\n",
    "    )\n",
    "    generation_time = time.time() - generation_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cf55a",
   "metadata": {},
   "source": [
    "## 8. Decode and Display Output\n",
    "\n",
    "Decode the generated tokens and display the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7cf3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant Response: As of my last update, here are some key areas where significant advancements have been made in AI:\n",
      "\n",
      "1. Natural Language Processing (NLP): Recent advancements in NLP include improved sentiment analysis, better handling of context, and more accurate translation between languages.\n",
      "\n",
      "2. Computer Vision: There has been progress in object recognition, facial recognition, and action recognition. Autonomous vehicles also rely heavily on computer vision for navigation.\n",
      "\n",
      "3. Machine Learning Algorithms: Techniques like reinforcement learning and unsupervised learning have gained popularity. These allow AI systems to learn from their environment without explicit instructions.\n",
      "\n",
      "4. Robotics: AI is being increasingly used in robotics, enabling machines to perform tasks independently or even autonomously.\n",
      "\n",
      "5. Healthcare: AI applications range from drug discovery to personalized medicine. AI algorithms can analyze medical images more accurately than human doctors, making early detection of diseases easier.\n",
      "\n",
      "6. Quantum Computing: As quantum computing technology advances, it could potentially solve complex problems that are currently unsolvable by classical computers, including those involving AI.\n",
      "\n",
      "7. Generative Models: There have been improvements in generative models like GPT-3 and DALL-E, which can generate realistic text and images respectively.\n",
      "\n",
      "8. Ethical and Fair AI: There's growing attention on developing AI technologies that respect privacy, fairness, transparency, and accountability.\n",
      "\n",
      "9. AI Assistants: Virtual assistants like Siri, Alexa, and Google Assistant continue to evolve, improving their ability to understand and respond to user queries.\n",
      "\n",
      "10. Explainable AI: As AI becomes more integrated into decision-making processes, there's an increasing need for explainable AI, which makes it possible to understand how AI systems make decisions.\n",
      "\n",
      "These are just a few examples. The field of AI is rapidly evolving, with new breakthroughs happening almost daily.\n"
     ]
    }
   ],
   "source": [
    "# Measure decoding time\n",
    "decoding_start = time.time()\n",
    "response = tokenizer.decode(\n",
    "    chat_outputs[0][chat_input['input_ids'].shape[-1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "decoding_time = time.time() - decoding_start\n",
    "\n",
    "print(\"\\nAssistant Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dddb40",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics\n",
    "\n",
    "Calculate and display various performance metrics including processing times and generation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4c97b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "Tokenization time: 0.01s\n",
      "Generation time: 29.20s\n",
      "Decoding time: 0.01s\n",
      "Total time: 29.22s\n",
      "Input tokens: 26\n",
      "Output tokens: 354\n",
      "Generation speed: 12.12 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "total_time = tokenization_time + generation_time + decoding_time\n",
    "num_input_tokens = chat_input['input_ids'].shape[1]\n",
    "num_output_tokens = chat_outputs[0].shape[0] - num_input_tokens\n",
    "tokens_per_second = num_output_tokens / generation_time\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Tokenization time: {tokenization_time:.2f}s\")\n",
    "print(f\"Generation time: {generation_time:.2f}s\")\n",
    "print(f\"Decoding time: {decoding_time:.2f}s\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Input tokens: {num_input_tokens}\")\n",
    "print(f\"Output tokens: {num_output_tokens}\")\n",
    "print(f\"Generation speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
