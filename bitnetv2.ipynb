{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08647ea2",
   "metadata": {},
   "source": [
    "# BitNet Model Implementation with Performance Optimizations\n",
    "\n",
    "This notebook demonstrates how to load and run the BitNet model with various optimizations for improved performance. We'll walk through each step of the process, from setting up the environment to generating and evaluating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8f56b0",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-req-build-pekd55f3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-req-build-pekd55f3\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 31f8a0fe8a7e2db1ee30bf32ed5976cd11f3283c\n",
      "  Installing build dependencies ... \u001b[?25l  Resolved https://github.com/huggingface/transformers.git to commit 31f8a0fe8a7e2db1ee30bf32ed5976cd11f3283c\n",
      "  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25done\n",
      "\u001b[?25hRequirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (1.15.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2025.4.26)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25lRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers==4.53.0.dev0) (2025.4.26)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l-done\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11365971 sha256=40ba125ce6ffcd24e813ced51a67bdcc0202d258a10a1998a4c379630b9bdec6\n",
      "  Stored in directory: /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-ephem-wheel-cache-4v1cgvgr/wheels/54/cb/3f/83103de5575c534436d6a4686686dead458238dfaf1147e98d\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11365971 sha256=40ba125ce6ffcd24e813ced51a67bdcc0202d258a10a1998a4c379630b9bdec6\n",
      "  Stored in directory: /private/var/folders/pt/hmd3x2w503g0phnxg6dz9s1h0000gn/T/pip-ephem-wheel-cache-4v1cgvgr/wheels/54/cb/3f/83103de5575c534436d6a4686686dead458238dfaf1147e98d\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.0.dev0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.0.dev0\n",
      "    Uninstalling transformers-4.52.0.dev0:\n",
      "      Successfully uninstalled transformers-4.52.0.dev0\n",
      "    Uninstalling transformers-4.52.0.dev0:\n",
      "      Successfully uninstalled transformers-4.52.0.dev0\n",
      "Successfully installed transformers-4.53.0.dev0\n",
      "Successfully installed transformers-4.53.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install torch git+https://github.com/huggingface/transformers.git bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c932d",
   "metadata": {},
   "source": [
    "## 1. Set Environment Variables\n",
    "\n",
    "First, we'll set up the necessary environment variables for tokenizer parallelism and PyTorch CPU memory alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25687c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set tokenizer parallelism before importing tokenizer-related modules\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Enable memory alignment\n",
    "os.environ['PYTORCH_CPU_ALLOC_CONF'] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d05296",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import the necessary Python libraries and enable PyTorch optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad26db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitNetConfig\n",
    "import torch.backends.cpu\n",
    "import time\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cpu.optimize = True\n",
    "torch.set_num_threads(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505ea07",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Model\n",
    "\n",
    "Load the BitNet tokenizer and set up padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/bitnet-b1.58-2B-4T\"\n",
    "\n",
    "# Load tokenizer and set up padding token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb33ef",
   "metadata": {},
   "source": [
    "## 4. Configure BitNet Model\n",
    "\n",
    "Set up the model configuration with various optimizations for improved inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5ec19c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BitNetForCausalLM(\n",
       "    (model): BitNetModel(\n",
       "      (embed_tokens): Embedding(128256, 2560, padding_idx=128009)\n",
       "      (layers): ModuleList(\n",
       "        (0-29): 30 x BitNetDecoderLayer(\n",
       "          (self_attn): BitNetAttention(\n",
       "            (q_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (k_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
       "            (v_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
       "            (o_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (attn_sub_norm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "          )\n",
       "          (mlp): BitNetMLP(\n",
       "            (gate_proj): AutoBitLinear(in_features=2560, out_features=6912, bias=False)\n",
       "            (up_proj): AutoBitLinear(in_features=2560, out_features=6912, bias=False)\n",
       "            (down_proj): AutoBitLinear(in_features=6912, out_features=2560, bias=False)\n",
       "            (act_fn): ReLUSquaredActivation()\n",
       "            (ffn_sub_norm): BitNetRMSNorm((6912,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "          (post_attention_layernorm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): BitNetRMSNorm((2560,), eps=1e-05)\n",
       "      (rotary_emb): BitNetRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize BitNet configuration with optimizations\n",
    "config = BitNetConfig.from_pretrained(model_id)\n",
    "config.low_cpu_mem_usage = True\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Performance optimizations\n",
    "config.use_cache = True  # Enable KV-cache for faster inference\n",
    "config.pretraining_tp = 1  # Tensor parallelism degree\n",
    "config.max_position_embeddings = 2048  # Match with your max_length\n",
    "config.hidden_dropout_prob = 0  # Disable dropout for inference\n",
    "config.attention_dropout_prob = 0  # Disable attention dropout for inference\n",
    "config.use_memory_efficient_attention = True  # Use memory efficient attention\n",
    "config.scale_attention_softmax_in_fp32 = False  # Keep in lower precision\n",
    "config.use_flash_attention = True  # Enable flash attention if supported\n",
    "\n",
    "# Load model with optimized configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,  # Explicitly set dtype to avoid ValueError\n",
    "    device_map='auto'  # Automatically handle device placement\n",
    ")\n",
    "\n",
    "# Move model to CPU and optimize\n",
    "model = model.cpu()\n",
    "model = torch.compile(model)  # Enable torch.compile for faster execution\n",
    "model.eval()  # Set to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e6f65",
   "metadata": {},
   "source": [
    "## 5. Prepare Input Messages\n",
    "\n",
    "Define the chat messages and apply the chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80047deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for processing\n",
    "BATCH_SIZE = 1  # Adjust based on your CPU memory\n",
    "\n",
    "# Prepare chat messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the latest advancements in AI.\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037682ec",
   "metadata": {},
   "source": [
    "## 6. Tokenize Input\n",
    "\n",
    "Tokenize the input messages and measure the tokenization time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b7e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():  # More efficient than no_grad for inference\n",
    "    # Measure tokenization time\n",
    "    tokenization_start = time.time()\n",
    "    chat_input = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust based on your needs\n",
    "    )\n",
    "    tokenization_time = time.time() - tokenization_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0511195",
   "metadata": {},
   "source": [
    "## 7. Generate Response\n",
    "\n",
    "Generate the model response with optimized generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb21e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bharathvjs/CodeSpace/1bit/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:684: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    # Measure generation time\n",
    "    generation_start = time.time()\n",
    "    chat_outputs = model.generate(\n",
    "        **chat_input,\n",
    "        max_new_tokens=500,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # Slightly higher temperature for faster sampling\n",
    "        top_p=0.95,      # Slightly higher top_p for faster sampling\n",
    "        top_k=40,        # Add top_k sampling for better speed/quality balance\n",
    "        use_cache=True,  # Enable KV-cache\n",
    "        num_beams=1,     # Disable beam search for faster generation\n",
    "        early_stopping=True,  # Enable early stopping\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,  # Light penalty to avoid repetitions efficiently\n",
    "        length_penalty=1.0,  # Neutral length penalty for faster completion\n",
    "        no_repeat_ngram_size=3  # Prevent repetition of 3-grams\n",
    "    )\n",
    "    generation_time = time.time() - generation_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cf55a",
   "metadata": {},
   "source": [
    "## 8. Decode and Display Output\n",
    "\n",
    "Decode the generated tokens and display the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7cf3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant Response: As of my last update, here are some significant recent advancements in artificial intelligence:\n",
      "\n",
      "1. **Generative Adversarial Networks (GANs):** GANs have made remarkable progress in generating high-quality images and videos. They've been used to create realistic art pieces, enhance medical imaging for better diagnosis, and even generate human-like text.\n",
      "\n",
      "2. **Natural Language Processing (NLP):** With improvements in NLP models like GPT-3, chatbots and virtual assistants have become more sophisticated. These systems can now understand complex queries and provide detailed responses.\n",
      "\n",
      "3. **Deep Learning:** The use of deep learning has seen tremendous growth, especially in areas such as computer vision, speech recognition, and natural language processing. \n",
      "\n",
      "4. **Autonomous Systems:** Advancements in machine learning and sensor technology have led to more autonomous vehicles, drones, and robots that can perform tasks with greater precision and safety.\n",
      "\n",
      "5. **AI in Healthcare:** AI is being used to analyze vast amounts of data from genomic sequences, helping doctors predict diseases earlier. It's also aiding in drug discovery by predicting potential side effects or interactions between different drugs.\n",
      "\n",
      "6. **Robotics:** Robotics has seen advancements in fields like manufacturing, where robots can learn to handle objects without direct programming. This is known as \"learning from demonstration\" or \"imitation learning.\"\n",
      "\n",
      "7. **Emotion Detection:** Newer algorithms are able to detect and interpret emotions from facial expressions, voice tone, and other cues. This technology is increasingly used in mental health care and customer service.\n",
      "\n",
      "8. **Ethical AI:** There's growing concern over the ethical implications of AI. Researchers are working on developing guidelines for responsible AI development and deployment to prevent misuse and ensure fairness.\n",
      "\n",
      "Remember, this information may change rapidly as new research and developments continue to emerge in the field of AI!\n"
     ]
    }
   ],
   "source": [
    "# Measure decoding time\n",
    "decoding_start = time.time()\n",
    "response = tokenizer.decode(\n",
    "    chat_outputs[0][chat_input['input_ids'].shape[-1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "decoding_time = time.time() - decoding_start\n",
    "\n",
    "print(\"\\nAssistant Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dddb40",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics\n",
    "\n",
    "Calculate and display various performance metrics including processing times and generation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4c97b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics:\n",
      "Tokenization time: 0.02s\n",
      "Generation time: 33.94s\n",
      "Decoding time: 0.02s\n",
      "Total time: 33.98s\n",
      "Input tokens: 26\n",
      "Output tokens: 369\n",
      "Generation speed: 10.87 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance metrics\n",
    "total_time = tokenization_time + generation_time + decoding_time\n",
    "num_input_tokens = chat_input['input_ids'].shape[1]\n",
    "num_output_tokens = chat_outputs[0].shape[0] - num_input_tokens\n",
    "tokens_per_second = num_output_tokens / generation_time\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Tokenization time: {tokenization_time:.2f}s\")\n",
    "print(f\"Generation time: {generation_time:.2f}s\")\n",
    "print(f\"Decoding time: {decoding_time:.2f}s\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Input tokens: {num_input_tokens}\")\n",
    "print(f\"Output tokens: {num_output_tokens}\")\n",
    "print(f\"Generation speed: {tokens_per_second:.2f} tokens/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
